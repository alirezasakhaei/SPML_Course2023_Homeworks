{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alirezasakhaei/SPML_Course2023_Homeworks/blob/main/HW01/nn_numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooJtNr5dGrH9"
      },
      "source": [
        "**Name:** Alireza Sakhaeirad\n",
        "\n",
        "**Student Number:** 98101714\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJm9Z1k0cdmh"
      },
      "source": [
        "# Neural-Network with Numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDN075MYGesD"
      },
      "source": [
        "In this notebook, you are going to write and implement all the components required to create and train a two-layered neural network using NumPy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt3FdxgNcdmm"
      },
      "source": [
        "## Imports & Seeding:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPZ4zlnxqhl5"
      },
      "source": [
        "Importing some common libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Et7OS7TGcdmn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "np.random.seed(123)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa2v2-xbcdmo"
      },
      "source": [
        "## Preparing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKWqV2Gycdmp"
      },
      "source": [
        "You'll train and evaluate your model on [Fashion MNIST](https://en.wikipedia.org/wiki/Fashion_MNIST) dataset. In this section, you'll download Fashion MNIST and split it into training and testing datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tMYZtSoLc7c-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6f0b938-0520-4d33-f58a-6ca975f42454"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/datasets/_openml.py:932: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(70000, 28, 28) (70000, 1)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Using `fetch_openml`, download `Fashion-MNIST` \n",
        "# and save the training data and labels in `X` and `y` respectively.\n",
        "#############################\n",
        "# Your code goes here (5 points)\n",
        "data = fetch_openml(data_id = 40996)\n",
        "X = data['data'].to_numpy().reshape(-1, 28, 28)\n",
        "y = data['target'].to_numpy().reshape(-1, 1).astype(np.int32)\n",
        "#############################\n",
        "\n",
        "# Normalization:\n",
        "X = ((X / 255.) - .5) * 2\n",
        "\n",
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sDmxyMJ4dBk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e08c8de4-89ee-4d7e-8f66-f6f27d4844b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28) (60000, 1) (10000, 28, 28) (10000, 1)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Using `train_test_split`, split your data into two sets. \n",
        "# Set the test_size to 10000\n",
        "\n",
        "#############################\n",
        "# Your code goes here (6 points)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size= 10000)\n",
        "#############################\n",
        "\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiGTXGXKcdmt"
      },
      "source": [
        "## Prepare training & validation sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba3nNYlDcdmt"
      },
      "source": [
        "We'll use only 3 classes from Fashion MNIST: Trouser, T-shirt, and Sneaker classes.\n",
        "\n",
        "The class labels for T-shirt, Trouser, and Sneaker are 0, 1, and 7 respectively.\n",
        "\n",
        "In this part, you'll limit the testing and training sets to only these three classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TcBDZEtzcdmu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07323033-c60e-4fc9-b657-d3c16c7928dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18017, 28, 28) (18017, 1)\n"
          ]
        }
      ],
      "source": [
        "# Modify `y_train` and `x_train`.\n",
        "# Only keep the 3 classes mentioned above. \n",
        "#############################\n",
        "# Your code goes here (4 points)\n",
        "train_indices = [True if y in [0, 1, 7] else False for y in y_train]\n",
        "train_indices = np.array(train_indices)\n",
        "\n",
        "if x_train.shape[0] == 60000:\n",
        "    x_train = x_train[train_indices]\n",
        "    y_train = y_train[train_indices]\n",
        "\n",
        "ind7 = np.where(y_train==7)[0]\n",
        "y_train[ind7]=2\n",
        "#############################\n",
        "\n",
        "print(x_train.shape, y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LX2hkRe1cdmw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c97f44d1-f9ce-4530-8c4b-be6eb8aaf263"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2983, 28, 28) (2983, 1)\n"
          ]
        }
      ],
      "source": [
        "# Modify `y_test` and `x_test`.\n",
        "# Only keep the 3 classes mentioned above. \n",
        "#############################\n",
        "# Your code goes here (4 points)\n",
        "test_indices = [True if y in [0, 1, 7] else False for y in y_test]\n",
        "test_indices = np.array(test_indices)\n",
        "\n",
        "if x_test.shape[0] == 10000:\n",
        "    x_test = x_test[test_indices]\n",
        "    y_test = y_test[test_indices]\n",
        "#############################\n",
        "\n",
        "ind7 = np.where(y_test==7)[0]\n",
        "y_test[ind7]=2\n",
        "\n",
        "print(x_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert x_test.shape[0] + x_train.shape[0] == 70000 * 3 // 10"
      ],
      "metadata": {
        "id": "6zvGrGyI3l3H"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv6SMLUktWbv"
      },
      "source": [
        "## Linear & Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXlyJo5JteKC"
      },
      "source": [
        "In this part, you'll implement the forward and backward process for the following components:\n",
        "- Softmax Layer\n",
        "- Linear Layer\n",
        "- ReLU Layer\n",
        "- Sigmoid Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXtAD5uYA4sQ"
      },
      "source": [
        "### The `Softmax` Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tzaIVo-_Axp7"
      },
      "outputs": [],
      "source": [
        "class SoftMaxLayer(object):\n",
        "    def __init__(self):\n",
        "        self.inp = None\n",
        "        self.output = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Write the forward pass for softmax.\n",
        "        # Save the values required for the backward pass.\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "        self.inp = x\n",
        "        # x = x - np.max(x)\n",
        "        if np.max(x)>100:\n",
        "            x=x-np.max(x)\n",
        "        elif np.min(x)<-100:\n",
        "            x=x+np.min(x)\n",
        "        self.output = np.exp(x) / (np.sum(np.exp(x))) \n",
        "        #############################\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, up_grad):\n",
        "        # Write the backward pass for softmax.\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "        up_grad = up_grad.reshape(-1, 1)\n",
        "        p = self.output  \n",
        "        local_grad_mat = np.diag(p) - p.reshape((-1, 1)) @ p.reshape((1, -1))\n",
        "        down_grad = local_grad_mat @ up_grad\n",
        "        return down_grad\n",
        "        #############################\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "\n",
        "    def step(self, optimizer):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcFoIDZjcdnB"
      },
      "source": [
        "### The `Linear` Layer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1strsTh6cdnG"
      },
      "outputs": [],
      "source": [
        "class Linear:\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        # Initialize the layer's weights and biases\n",
        "        #############################\n",
        "        # Your code goes here (2 points)\n",
        "        self.W = np.random.random((out_dim, in_dim))-0.5\n",
        "        self.b = np.random.random((out_dim, 1))-0.5\n",
        "        #############################\n",
        "        self.dw = 0\n",
        "        self.db = 0\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Compute linear layer's output.\n",
        "        # Save the value(s) required for the backward phase.\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "        # print('----')\n",
        "        # print(np.max(x), np.min(x))\n",
        "        x = x.reshape(-1, 1)\n",
        "        self.inp = x\n",
        "        z = self.W @ x + self.b\n",
        "        # print(np.max(z), np.min(z))\n",
        "        #############################\n",
        "        return z\n",
        "    \n",
        "    def backward(self, up_grad):\n",
        "        # Calculate the gradient with respect to the weights \n",
        "        # and biases and save the results.\n",
        "        #############################\n",
        "        # Your code goes here (6 points)\n",
        "        up_grad = up_grad.reshape(-1, 1)\n",
        "        self.db += up_grad\n",
        "        self.dw += up_grad @ self.inp.reshape(1, -1)\n",
        "        # print(up_grad.shape)\n",
        "        down_grad = up_grad.reshape(1, -1) @ self.W\n",
        "        #############################\n",
        "        return down_grad\n",
        "    \n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "    def step(self, optimizer):\n",
        "        # Update the layer's weights and biases\n",
        "        # Update previous_w_update and previous_b_update accordingly\n",
        "        #############################\n",
        "        # Your code goes here (5 points)\n",
        "        self.W = optimizer.get_next_update(self.W, self.dw)\n",
        "        self.b = optimizer.get_next_update(self.b, self.db)\n",
        "        #############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0Lfo-nhcdnG"
      },
      "source": [
        "### The `ReLU` Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tN6vcirMcdnH"
      },
      "outputs": [],
      "source": [
        "class ReLU:\n",
        "    def __init__(self):\n",
        "        self.inp = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Write the forward pass for ReLU.\n",
        "        # Save the value(s) required for the backward pass.\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "        x = x.reshape(-1, 1)\n",
        "        self.inp = x\n",
        "        output = np.maximum(x, 0)\n",
        "        #############################\n",
        "        return output\n",
        "    \n",
        "    def backward(self, up_grad):\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "        up_grad = up_grad.reshape(-1, 1)\n",
        "        local_grad = np.heaviside(self.inp, 0)\n",
        "        down_grad = np.multiply(up_grad, local_grad)\n",
        "        #############################\n",
        "        return down_grad\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "        \n",
        "    def step(self, optimizer):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z00KoSI3cdnJ"
      },
      "source": [
        "### The `sigmoid` Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TTYYeL2lcdnJ"
      },
      "outputs": [],
      "source": [
        "class Sigmoid:\n",
        "    def forward(self, x):\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "        self.inp = x\n",
        "        self.out = np.divide(1, 1+np.exp(-1 * x))\n",
        "        #############################\n",
        "        return self.out\n",
        "    \n",
        "    def backward(self, up_grad):\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "        up_grad = up_grad.reshape(-1, 1)\n",
        "        temp = self.out.reshape(-1, 1)\n",
        "        local_grad = temp - np.power(temp, 2)\n",
        "        # local_grad = np.mean(local_grad, axis=1)\n",
        "        # print(local_grad.shape)\n",
        "        down_grad = np.multiply(up_grad, local_grad)\n",
        "        #############################\n",
        "        return down_grad\n",
        "    \n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "    def step(self, optimizer):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " x = np.array([1, 2, 3])\n",
        " s = Sigmoid()\n",
        " s(x)"
      ],
      "metadata": {
        "id": "xazKL3MRV1m1",
        "outputId": "5ef701c8-9308-4eb3-dbfe-a7cf02a4e948",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.73105858, 0.88079708, 0.95257413])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s.backward(np.array([1,1,1]))"
      ],
      "metadata": {
        "id": "jsccTTa1XMCa",
        "outputId": "99f59de7-90af-4b68-e4d8-fd8f6735a56f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.19661193],\n",
              "       [0.10499359],\n",
              "       [0.04517666]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zngleGY2cdnK"
      },
      "source": [
        "## `Loss` function :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISedT4FvcdnK"
      },
      "source": [
        "For this task we are going to use the [Cross-Entropy Loss](https://en.wikipedia.org/wiki/Cross_entropy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "XQyz4ybycdnL"
      },
      "outputs": [],
      "source": [
        "class CELoss():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        self.yhat = pred\n",
        "        self.y = target\n",
        "        # m = self.y.shape[0]\n",
        "        # m = 1\n",
        "        # Commpute and return the loss \n",
        "        #############################\n",
        "        # Your code goes here (8 points)\n",
        "        # self.m = m\n",
        "        self.y = self.y.reshape(-1, 1)\n",
        "        self.yhat = self.yhat.reshape(-1, 1)\n",
        "        epsilon = 1e-8\n",
        "        yhat_log = np.log(self.yhat + epsilon).reshape(-1, 1)\n",
        "        loss = np.sum(np.multiply(-self.y, yhat_log))\n",
        "        # loss = loss / m\n",
        "        self.loss = loss\n",
        "        return self.loss\n",
        "        #############################\n",
        "        \n",
        "    def __call__(self, pred, target):\n",
        "        return self.forward(pred, target)\n",
        "\n",
        "    def backward(self):\n",
        "        # Derivative of loss_fn with respect to a the predicted label.\n",
        "        # Use `self.y` and `self.yhat` to compute and return `grad`.\n",
        "        #############################\n",
        "        # Your code goes here (6 points)\n",
        "        # m = self.m\n",
        "        epsilon = 1e-8\n",
        "        self.y = self.y.reshape(-1, 1)\n",
        "        grad = -np.divide(self.y, self.yhat + epsilon)\n",
        "        # grad = np.mean(grad, axis=1)\n",
        "        grad = grad.reshape(-1, 1)\n",
        "        #############################\n",
        "        return grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xovZI-70kB9I"
      },
      "source": [
        "## Optimizer\n",
        "\n",
        "In this section, you'll implement an optimizer classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "h5ADTi5tkVTS"
      },
      "outputs": [],
      "source": [
        "class GradientDescent(object):\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "\n",
        "    def get_next_update(self, x, dx):\n",
        "        # Compute the new value for 'x' and return the result\n",
        "        #############################\n",
        "        # Your code goes here (2 points)\n",
        "        x = x - self.lr * (dx/np.linalg.norm(dx))\n",
        "        return x\n",
        "        #############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxxrEEovYEFi"
      },
      "source": [
        "## The Model\n",
        "Now you'll write the base class for a multi-layer perceptron network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "t8SoZeYRcdnY"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    def __init__(self, layers, loss_fn, optimizer):\n",
        "        self.layers = layers \n",
        "        self.losses  = [] \n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for layer in self.layers:\n",
        "            if type(layer) == type(Linear(1, 1)):\n",
        "                layer.dw = 0\n",
        "                layer.db = 0\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # Pass `inp` to all the layers sequentially\n",
        "        # and return the result.\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "        # print(inp)\n",
        "        for layer in self.layers:\n",
        "            # print('-------------------------')\n",
        "            inp = layer(inp)\n",
        "            # print(inp)\n",
        "        return inp\n",
        "        #############################\n",
        "        \n",
        "    def loss(self, pred, label):\n",
        "        loss = self.loss_fn.forward(pred, label)\n",
        "        return loss\n",
        "\n",
        "    def backward(self):\n",
        "        # Start with loss function's gradient and \n",
        "        # do the backward pass on all the layers.\n",
        "        #############################\n",
        "        # Your code goes here (5 points)\n",
        "        down_grad = self.loss_fn.backward()\n",
        "        for layer in reversed(self.layers):\n",
        "            down_grad = layer.backward(down_grad)\n",
        "        #############################\n",
        "        \n",
        "    def update(self):\n",
        "        for layer in self.layers:\n",
        "            layer.step(self.optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo0rNwYciueF"
      },
      "source": [
        "The following cell encodes training labels into a one-hot representation with 3 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "nhJTulaFJ4vR"
      },
      "outputs": [],
      "source": [
        "def onehot_enc(y, num_labels):\n",
        "    ary = np.zeros((y.shape[0], num_labels))\n",
        "    for i, val in enumerate(y):\n",
        "        ary[i, val] = 1\n",
        "    return ary\n",
        "\n",
        "y_train = onehot_enc(y_train, 3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.reshape(-1, 784)\n",
        "x_test = x_test.reshape(-1, 784)"
      ],
      "metadata": {
        "id": "MHFro_kKdBaV"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "TS6S_RUwsRkF"
      },
      "outputs": [],
      "source": [
        "def train(model, epochs, x, y):\n",
        "    losses = []\n",
        "    model.zero_grad()\n",
        "    for n in tqdm(range(epochs)):\n",
        "        # First do the forward pass. Next, compute the loss.\n",
        "        # Then do the backward pass and finally, update the parameters.\n",
        "        #############################\n",
        "        # Your code goes here (4 points)\n",
        "        loss = 0\n",
        "        for i in range(x_train.shape[0]):\n",
        "\n",
        "            x = x_train[i]\n",
        "            y = y_train[i]\n",
        "            yhat = model.forward(x)\n",
        "            loss += model.loss(yhat, y)\n",
        "            model.backward()\n",
        "\n",
        "            if (i%1024==0):\n",
        "                model.update()\n",
        "                model.zero_grad()\n",
        "\n",
        "        acc = calc_acc(model)\n",
        "        loss = loss / x_train.shape[0]\n",
        "        losses.append(loss)\n",
        "        #############################\n",
        "        print(f\"Loss and Acc at {n}: {loss:.3f}, {acc:.3f}\")\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l=Linear(784,50)\n",
        "l2=ReLU()\n",
        "l3=Linear(50,50)\n",
        "l4=Linear(50,3)\n",
        "l5=SoftMaxLayer()\n"
      ],
      "metadata": {
        "id": "02ryd4-MmViH"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l5(l4(l2(l3(l2(l(x_train[0]))))))"
      ],
      "metadata": {
        "id": "-abiCd4xojSN",
        "outputId": "50bf1435-ed48-455b-ba8c-934b4a0600c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.82974933],\n",
              "       [0.05842853],\n",
              "       [0.11182214]])"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "m1lSq2jNcdnY",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e51fbff3-767a-45d2-b61e-dd3fc9c9813a"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1/50 [00:11<09:21, 11.46s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 0: 1.130, 0.369\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|▍         | 2/50 [00:22<09:05, 11.37s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 1: 1.136, 0.374\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  6%|▌         | 3/50 [00:35<09:31, 12.16s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 2: 1.147, 0.379\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  8%|▊         | 4/50 [00:47<09:02, 11.80s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 3: 1.162, 0.387\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 5/50 [00:58<08:46, 11.71s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 4: 1.177, 0.393\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 12%|█▏        | 6/50 [01:10<08:32, 11.64s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 5: 1.186, 0.401\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 14%|█▍        | 7/50 [01:21<08:17, 11.58s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 6: 1.187, 0.409\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 16%|█▌        | 8/50 [01:32<08:00, 11.44s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 7: 1.177, 0.417\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 18%|█▊        | 9/50 [01:42<07:30, 10.99s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 8: 1.157, 0.419\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 10/50 [01:54<07:26, 11.17s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 9: 1.134, 0.420\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 22%|██▏       | 11/50 [02:05<07:19, 11.26s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 10: 1.115, 0.420\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 24%|██▍       | 12/50 [02:17<07:09, 11.30s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 11: 1.105, 0.422\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 26%|██▌       | 13/50 [02:28<06:58, 11.31s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 12: 1.099, 0.424\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 28%|██▊       | 14/50 [02:39<06:47, 11.33s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 13: 1.096, 0.424\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 15/50 [02:50<06:29, 11.13s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 14: 1.094, 0.427\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 32%|███▏      | 16/50 [03:00<06:08, 10.84s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 15: 1.094, 0.430\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 34%|███▍      | 17/50 [03:12<06:04, 11.05s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 16: 1.094, 0.430\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 36%|███▌      | 18/50 [03:23<05:56, 11.13s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 17: 1.094, 0.430\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 38%|███▊      | 19/50 [03:35<05:49, 11.26s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 18: 1.095, 0.434\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 20/50 [03:47<05:49, 11.65s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 19: 1.096, 0.443\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 42%|████▏     | 21/50 [03:59<05:41, 11.76s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 20: 1.097, 0.454\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 44%|████▍     | 22/50 [04:11<05:27, 11.70s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 21: 1.097, 0.463\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 46%|████▌     | 23/50 [04:22<05:13, 11.63s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 22: 1.097, 0.468\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 48%|████▊     | 24/50 [04:32<04:48, 11.10s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 23: 1.098, 0.476\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 25/50 [04:43<04:39, 11.18s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 24: 1.098, 0.479\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 52%|█████▏    | 26/50 [04:55<04:29, 11.25s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 25: 1.098, 0.478\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 54%|█████▍    | 27/50 [05:06<04:19, 11.27s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 26: 1.098, 0.482\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 56%|█████▌    | 28/50 [05:18<04:11, 11.42s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 27: 1.098, 0.485\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 58%|█████▊    | 29/50 [05:29<03:59, 11.43s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 28: 1.098, 0.485\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 30/50 [05:41<03:47, 11.38s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 29: 1.098, 0.481\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 62%|██████▏   | 31/50 [05:51<03:27, 10.93s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 30: 1.099, 0.479\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 64%|██████▍   | 32/50 [06:02<03:16, 10.94s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 31: 1.099, 0.478\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 66%|██████▌   | 33/50 [06:13<03:08, 11.10s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 32: 1.099, 0.477\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 68%|██████▊   | 34/50 [06:24<02:59, 11.20s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 33: 1.099, 0.477\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 35/50 [06:36<02:50, 11.35s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 34: 1.099, 0.476\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 72%|███████▏  | 36/50 [06:47<02:37, 11.26s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss and Acc at 35: 1.099, 0.474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74%|███████▍  | 37/50 [06:58<02:24, 11.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss and Acc at 36: 1.099, 0.474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|███████▌  | 38/50 [07:10<02:17, 11.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss and Acc at 37: 1.099, 0.473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|███████▊  | 39/50 [07:21<02:04, 11.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss and Acc at 38: 1.099, 0.473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 40/50 [07:31<01:48, 10.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss and Acc at 39: 1.099, 0.475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|████████▏ | 41/50 [07:42<01:37, 10.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss and Acc at 40: 1.099, 0.481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|████████▍ | 42/50 [07:53<01:26, 10.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss and Acc at 41: 1.099, 0.481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%|████████▌ | 43/50 [08:03<01:15, 10.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss and Acc at 42: 1.099, 0.484\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|████████▊ | 44/50 [08:14<01:04, 10.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss and Acc at 43: 1.099, 0.488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 45/50 [08:25<00:54, 10.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss and Acc at 44: 1.099, 0.494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|█████████▏| 46/50 [08:36<00:43, 10.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss and Acc at 45: 1.099, 0.498\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|█████████▍| 47/50 [08:47<00:32, 10.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss and Acc at 46: 1.099, 0.502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96%|█████████▌| 48/50 [08:57<00:21, 10.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss and Acc at 47: 1.099, 0.509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 98%|█████████▊| 49/50 [09:07<00:10, 10.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss and Acc at 48: 1.099, 0.510\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [09:18<00:00, 11.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss and Acc at 49: 1.099, 0.514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the `MLP` with the following structure:\n",
        "#     Linear with 50 units --> ReLU --> Linear with 50 units --> ReLU --> Linear with 3 units --> Sigmoid --> Softmax\n",
        "# Use GradientDescent as the optimizer, set the learning rate to 0.001, and use CELoss as the loss function.\n",
        "#############################\n",
        "# Your code goes here (4 points)\n",
        "layers = [Linear(784, 50), ReLU(), Linear(50, 50), ReLU(), Linear(50, 3), Sigmoid(), SoftMaxLayer()]\n",
        "# layers = [Linear(784,3), SoftMaxLayer()]\n",
        "nn = MLP(layers, CELoss(), GradientDescent(0.001))\n",
        "#############################\n",
        "# nn.forward(x_train[100])\n",
        "epochs = 50\n",
        "\n",
        "\n",
        "# Train the network using only `x_train` and `y_train` (no validation)\n",
        "losses = train(nn, epochs, x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJec2xRJmY37"
      },
      "source": [
        "Let's plot the loss value for each iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "ymaQNn70cdnZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "f9320024-0a5c-4db1-b373-b9f1d6885708"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(losses)\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz3yqRa1cdna"
      },
      "source": [
        "**Let's also check our model's performance using the `accuracy` metric on the `testing` dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "TRqwXho7cdnd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29264829-7b85-4801-fd10-7e075e64a760"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5139121689574254\n"
          ]
        }
      ],
      "source": [
        "# Compute the accuracy on the testing set\n",
        "#############################\n",
        "# Your code goes here (7 points)\n",
        "def calc_acc(model):\n",
        "    c = 0\n",
        "    yhat = np.zeros(y_test.shape[0])\n",
        "    for i in range(yhat.shape[0]):\n",
        "        pred = model.forward(x_test[i])\n",
        "        pred = np.argmax(pred)\n",
        "        if int(pred)==y_test[i]:\n",
        "            c += 1\n",
        "\n",
        "    acc = c / y_test.shape[0]\n",
        "    return acc\n",
        "\n",
        "acc = calc_acc(nn)\n",
        "#############################\n",
        "\n",
        "print(acc)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}